save_dir: exp/valle/nar_llama
seed_everything: 42
data:
  data_dir: exp/egs/libritts
  file_patterns:
    train: cuts*train*
    val: cuts*dev*
  filter_min_dur: 1.
  filter_max_dur: 20
  max_duration: 100.0
  # max_cuts: 10
  bucketing_sampler: true
  num_buckets: 10
  concatenate_cuts: false
  duration_factor: 1.0
  gap: 0.1
  shuffle: true
  drop_last: true
  return_cuts: true
  num_workers: 8
  tokenizer_config:
    language: en-us
    backend: espeak
model:
  rope: true
  num_layers: 12
  num_heads: 16
  hidden_size: 1024
  dropout: 0.1
  ffn_type: gated
  hidden_act: swish
  bias: false
  norm_type: rms
  use_sdpa: true
  prefix_mode: starter
  has_ar: false
  has_nar: true
resume_ckpt: null
init_weight: false
init_weight_params:
  checkpoint: last.ckpt
  dirpath: null
  best_k_fname: best_k_models.yaml
  best_k_mode: min
teacher:
  optimizer:
  - adamw
  - weight_decay: 0.01
    betas: [0.9, 0.95]
  lr_scheduler:
  - eden
  - warmup_steps: 1000
    lr_batches: 5000
    lr_epochs: 4
    warmup_start: 0.1
  lr: 0.0005
trainer:
  accelerator: auto
  strategy: ddp_find_unused_parameters_true
  devices: 2,3
  precision: 16-mixed
  fast_dev_run: false
  max_epochs: 60
  limit_train_batches: null
  # limit_val_batches: 128
  val_check_interval: 5000
  log_every_n_steps: 100
  accumulate_grad_batches: 4
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  inference_mode: true
  use_distributed_sampler: false
  profiler: null
mckpt:
  filename: epoch{epoch}-val_loss{val_loss:.3f}-step{step}
  monitor: val_loss
  save_last: true
  save_top_k: 10
  mode: min
  auto_insert_metric_name: false
  every_n_epochs: 1
  # every_n_train_steps: 1000
  save_on_train_epoch_end: true

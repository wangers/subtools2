manifest_dir: miss
prefix_name: aishell_vi
suffix: jsonl.gz
max_cuts: 16
bucketing_sampler: true
num_buckets: 30
concatenate_cuts: false
duration_factor: 1.0
gap: 1.0
on_the_fly_feats: false
shuffle: true
enable_spec_aug: true
spec_aug_time_warp_factor: 80
enable_musan: true
base_model: openai/whisper-tiny
output_dir: output/whisper-tiny/lora
optim: adamw_torch
num_train_epochs: 3
max_steps: -1
warmup_steps: 1000
logging_steps: 100
eval_steps: 2500
save_steps: 5000
num_workers: 2
learning_rate: 0.001
use_adalora: true
fp16: true
use_8bit: false
use_compile: false
local_files_only: true
language: vi
task: transcribe
resume_from_checkpoint: null
gradient_accumulation_steps: 1
gradient_checkpointing: false
max_grad_norm: 5.
remove_whisper_encoder_input_length_restriction: false
empty_init: true
deepspeed: config/ds_config_z1.json

# type `egrecho train-asv -h` to show help
save_dir: './exp/ecapa'
seed_everything: 42
data_builder:
  class_path: ASVPipeBuilder
  init_args:
    config:
      class_path: ASVBuilderConfig
      init_args:
        data_dir: exp/egs/cnsrc_train
        file_patterns:
          train: egs*train*
          val: egs*val*
        data_type: raw
        shuffle: true
        partition: true
        label_fname: speaker.yaml
        resample_rate: 16000
        pre_sp_factors:
        - 0.9
        - 1.0
        - 1.1
        rand_chunksize: 200
        frame_shift: 0.01
        speech_aug: true
        speech_aug_config:
          batch_size: 1
          db_dir: /work/ldx/speech_aug
        shard_shuffle_size: 1500
        batch_size: 128
        drop_last: true
        extractor_param:
          mean_norm: true
          std_norm: false
          return_attention_mask: false
          feat_conf:
            feature_type: kaldi-fbank
data_attrs:
- inputs_dim
- num_classes
data:
  num_workers: 8
  prefetch_factor: 50
  val_num_workers: 0
  pin_memory: false
  fullsync: true
run:
  model:
    class_path: EcapaModel
    init_args:
      config:
        class_path: EcapaSVConfig
        init_args:
          inputs_dim: 80
          channels: 512
          embd_dim: 192
          mfa_dim: 1536
          pooling_params:
            num_q: 1
            num_head: 1
            time_attention: true
            hidden_size: 128
            stddev: true
          embd_layer_num: 1
          post_norm: false
          num_classes: 2
          head_name: aam
          head_params:
            sub_k: 1
            do_topk: false
  teacher:
    class_path: SVTeacher
    init_args:
      num_classes: null
      margin_warm: true
      margin_warm_kwargs:
        start_epoch: 5
        end_epoch: 15
      optimizer:
      - adamw
      - weight_decay: 0.05
      lr_scheduler:
      - warm_cosine
      - warmup_steps: 0.1
        pct_start: 0.1
      lr: 0.002
  trainer:
    accelerator: gpu
    strategy: auto
    devices: 4
    precision: 16-mixed
    fast_dev_run: false
    max_epochs: 60
    limit_val_batches: null
    val_check_interval: 500
    check_val_every_n_epoch: 1
    num_sanity_val_steps: null
    log_every_n_steps: 100
    accumulate_grad_batches: 1
    gradient_clip_val: 5.0
    gradient_clip_algorithm: norm
    deterministic: null
    benchmark: true
    inference_mode: true
    profiler: null
    sync_batchnorm: true
  mckpt:
    dirpath: null
    filename: epoch{epoch}-val_loss{val_loss:.4f}-step{step}
    monitor: val_loss
    save_last: true
    save_top_k: 10
    mode: min
    auto_insert_metric_name: false
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: 1
    save_on_train_epoch_end: true
  use_early_stopping: true
  early_stopping:
    monitor: val_loss
    min_delta: 0.0
    patience: 5
    mode: min
    check_on_train_epoch_end: true
    log_rank_zero_only: false


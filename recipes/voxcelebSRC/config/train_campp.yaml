# type `egrecho train-asv -h` to show help
save_dir: './exp/campp'
seed_everything: 42
data_builder:
  class_path: ASVPipeBuilder
  init_args:
    config:
      class_path: ASVBuilderConfig
      init_args:
        data_dir: exp/egs/voxceleb2_shard
        file_patterns:
          train: egs*train*
          val: egs*val*
        data_type: shard
        shuffle: true
        partition: true
        label_fname: speaker.yaml
        resample_rate: 16000
        pre_sp_factors:
        - 0.9
        - 1.0
        - 1.1
        rand_chunksize: 200
        chunk_retry_param:
          retry: 2
        frame_shift: 0.01
        speech_aug: true
        speech_aug_config:
          batch_size: 1
          db_dir: /data/speech_aug  # replace your noise set
          init_p: 0.5
          # sim_rir_prob: 0.5
          wave_drop: true
          drop_time_count: [0, 4]
        shard_shuffle_size: 1500
        batch_size: 256
        drop_last: true
        extractor_param:
          # scale_bit: 16
          mean_norm: true
          std_norm: false
          return_attention_mask: false
          feat_conf:
            feature_type: kaldi-fbank
data_attrs:
- inputs_dim
- num_classes
data:
  num_workers: 8
  prefetch_factor: 20
  val_num_workers: 0
  pin_memory: false
  fullsync: true
run:
  model:  # can replace a config yaml here
    class_path: CamPPModel
    init_args:
      config:
        class_path: CamPPSVConfig
        init_args:
          inputs_dim: 80
          memory_efficient: false
          classifier_str: aam
          classifier_params:
            sub_k: 1
            do_topk: false
  resume_ckpt: null
  teacher:
    class_path: SVTeacher
    init_args:
      num_classes: null
      margin_warm: true
      margin_warm_kwargs:
        start_epoch: 10
        end_epoch: 25
      optimizer:
      - sgd
      - weight_decay: 1e-4
        momentum: 0.9
        nesterov: true
      # - adamw
      # - weight_decay: 5e-2
      lr_scheduler:
      - warm_cosine
      - warmup_steps: 0.05
        # pct_start: 0.1
        eta_min: 1e-6
      lr: 0.1
  trainer:
    accelerator: gpu
    strategy: auto
    devices: 4
    precision: 32   # 16-mixed
    fast_dev_run: false
    max_epochs: 120
    limit_val_batches: null
    val_check_interval: 500
    check_val_every_n_epoch: 1
    num_sanity_val_steps: null
    log_every_n_steps: 100
    accumulate_grad_batches: 1
    gradient_clip_val: 5.0
    gradient_clip_algorithm: norm
    deterministic: null
    benchmark: true
    inference_mode: true
    profiler: null
    sync_batchnorm: true
  mckpt:
    dirpath: null
    filename: epoch{epoch}-val_loss{val_loss:.2f}-step{step:.2e}
    monitor: val_loss
    save_last: true
    save_top_k: 10
    mode: min
    auto_insert_metric_name: false
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: 1
    save_on_train_epoch_end: true
  use_early_stopping: false
  early_stopping:
    monitor: val_loss
    min_delta: 0.0
    patience: 10
    mode: min
    check_on_train_epoch_end: true
    log_rank_zero_only: true
